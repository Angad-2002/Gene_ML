{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA4lTJ7oaRWzTTLYUWLopE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angad-2002/Gene_ML/blob/main/Gene_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhvPM8klkvYs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def MNBC (file1, file2, file3):\n",
        "\n",
        "    S1, S2, S3 = \"\", \"\", \"\"\n",
        "\n",
        "    file1 = open (file1.name);\n",
        "    file2 = open (file2.name);\n",
        "    file3 = open (file3.name);\n",
        "\n",
        "    human_dna = pd.read_table (file1)\n",
        "    human_dna.head ()\n",
        "\n",
        "    chimp_dna = pd.read_table (file2)\n",
        "    chimp_dna.head ()\n",
        "\n",
        "    dog_dna = pd.read_table (file3)\n",
        "    dog_dna.head ()\n",
        "\n",
        "\n",
        "    #K - mer List function has been created. (K = 6 (hexamer))\n",
        "    def Kmers_funct (seq, size = 6):\n",
        "        return [seq[x:x + size].lower () for x in range (len (seq) - size + 1)]\n",
        "\n",
        "    #Applying the above function on each dataset.\n",
        "    human_dna['words'] = human_dna.apply (lambda x: Kmers_funct (x['sequence']), axis = 1)\n",
        "    human_dna = human_dna.drop ('sequence', axis = 1)\n",
        "    chimp_dna['words'] = chimp_dna.apply (lambda x: Kmers_funct (x['sequence']), axis = 1)\n",
        "    chimp_dna = chimp_dna.drop ('sequence', axis = 1)\n",
        "    dog_dna['words'] = dog_dna.apply (lambda x: Kmers_funct (x['sequence']), axis = 1)\n",
        "    dog_dna = dog_dna.drop ('sequence', axis = 1)\n",
        "\n",
        "    #Converting the K - mer list into string sentences of words for conversion into Bag of Words Model.\n",
        "    #Y labels are used as class labels.\n",
        "\n",
        "    human_texts = list (human_dna['words'])\n",
        "    for item in range (len (human_texts)):\n",
        "        human_texts[item] = ' '.join (human_texts[item])\n",
        "\n",
        "    y_human = human_dna.iloc[:, 0].values # y_human for human_dna\n",
        "\n",
        "\n",
        "    chimp_texts = list (chimp_dna['words'])\n",
        "    for item in range (len (chimp_texts)):\n",
        "        chimp_texts[item] = ' '.join (chimp_texts[item])\n",
        "\n",
        "    y_chim = chimp_dna.iloc[:, 0].values # y_chim for chimp_dna\n",
        "\n",
        "    dog_texts = list (dog_dna['words'])\n",
        "    for item in range (len (dog_texts)):\n",
        "        dog_texts[item] = ' '.join (dog_texts[item])\n",
        "\n",
        "    y_dog = dog_dna.iloc[:, 0].values  # y_dog for dog_dna\n",
        "\n",
        "\n",
        "    # Creating the Bag of Words model using CountVectorizer()\n",
        "    # This is equivalent to k-mer counting\n",
        "    # The n-gram size of 4 was previously determined by rigorous testing (hit and trial)\n",
        "\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    cv = CountVectorizer (ngram_range = (4,4))\n",
        "    X = cv.fit_transform (human_texts)\n",
        "    X_chimp = cv.transform (chimp_texts)\n",
        "    X_dog = cv.transform (dog_texts)\n",
        "\n",
        "    human_dna['class'].value_counts().sort_index().plot.bar()\n",
        "\n",
        "    chimp_dna['class'].value_counts().sort_index().plot.bar()\n",
        "\n",
        "    dog_dna['class'].value_counts ().sort_index ().plot.bar ()\n",
        "\n",
        "\n",
        "    #Now we will use the Classification model with 85% human dataset for training and 15% for testing.\n",
        "    #Then we will use the other datsets for testing.\n",
        "    #Lastly we will do performance analyis.\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split (X, y_human, test_size = 0.15, random_state = 42)\n",
        "\n",
        "\n",
        "    #By Rigourous trials of different values alpha = 0.1 was decided to be the optimal value.\n",
        "\n",
        "    from sklearn.naive_bayes import MultinomialNB\n",
        "    classifier = MultinomialNB (alpha=0.1)\n",
        "    classifier.fit (X_train, y_train)\n",
        "\n",
        "    #Now let's make predictions on the human hold out test set and see how it performes on unseen data.\n",
        "\n",
        "    y_pred = classifier.predict (X_test)\n",
        "\n",
        "    \"\"\"\n",
        "    Okay, so let's look at some model performance metrics like the confusion matrix, accuracy, precision, recall and f1 score. We are getting really good results on our unseen data, so it looks like our model did not overfit to the training data. In a real project I would go back and sample many more train test splits since we have a relatively small data set.\n",
        "\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "    D1 = pd.crosstab (pd.Series (y_test, name = 'Actual'), pd.Series (y_pred, name = 'Predicted'))\n",
        "\n",
        "    def get_metrics (y_test, y_predicted):\n",
        "        accuracy = accuracy_score (y_test, y_predicted)\n",
        "        precision = precision_score (y_test, y_predicted, average = 'weighted')\n",
        "        recall = recall_score (y_test, y_predicted, average = 'weighted')\n",
        "        f1 = f1_score (y_test, y_predicted, average = 'weighted')\n",
        "        return accuracy, precision, recall, f1\n",
        "\n",
        "    accuracy, precision, recall, f1 = get_metrics (y_test, y_pred)\n",
        "    S1 += \"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\\n\" % (accuracy, precision, recall, f1) + \"\\n\"\n",
        "\n",
        "\n",
        "    # Predicting the chimp and dog sequences.\n",
        "\n",
        "    y_pred_chimp = classifier.predict (X_chimp)\n",
        "    y_pred_dog = classifier.predict (X_dog)\n",
        "\n",
        "    # performance on chimp genes\n",
        "    D2 = pd.crosstab (pd.Series (y_chim, name = 'Actual'), pd.Series (y_pred_chimp, name = 'Predicted'))\n",
        "    accuracy, precision, recall, f1 = get_metrics (y_chim, y_pred_chimp)\n",
        "    S2 += \"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\\n\" % (accuracy, precision, recall, f1) + \"\\n\"\n",
        "\n",
        "    # performance on dog genes\n",
        "    D3 = pd.crosstab (pd.Series (y_dog, name = 'Actual'), pd.Series (y_pred_dog, name = 'Predicted'))\n",
        "    accuracy, precision, recall, f1 = get_metrics (y_dog, y_pred_dog)\n",
        "    S3 += \"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\\n\" % (accuracy, precision, recall, f1) + \"\\n\"\n",
        "\n",
        "    return (D1, S1, D2, S2, D3, S3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "input = [gr.File (label = \"Human Dataset\"), gr.File (label = \"Chimpanzee Dataset\"), gr.File (label = \"Dog Dataset\")]\n",
        "output = [gr.Dataframe (label = \"Confusion Matrix for Humans\"), gr.Text (label = \"Precision Metrics for Humans\"),\n",
        "          gr.Dataframe (label = \"Confusion Matrix for Chimpanzees\"), gr.Text (label = \"Precision Metrics for Chimpanzees\"),\n",
        "          gr.Dataframe (label = \"Confusion Matrix for Dogs\"), gr.Text (label = \"Precision Metrics for Dogs\")]\n",
        "interface = gr.Interface (fn = MNBC, inputs = input, outputs = output)\n",
        "interface.launch (share = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "H4ZP6ndkk0kv",
        "outputId": "60cb6cbd-e597-424c-e4d2-28f601d90b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://d1fc174cdc3783134b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d1fc174cdc3783134b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}